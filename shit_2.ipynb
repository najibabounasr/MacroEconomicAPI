{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MSE Scores:\n",
      "{'XGBoost': 0.0005103063958695216, 'LightGBM': 0.0005872492303105134}\n",
      "Aggregated Baseline MSE Score: 0.001097555626180035\n",
      "\n",
      "Best parameters saved to best_params.py.\n"
     ]
    }
   ],
   "source": [
    "# ADDING FEATURES, PT.1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "from funcs.api_funcs import get_rounds_arg, get_target_arg\n",
    "import random\n",
    "import os\n",
    "from funcs.engineer_features_funcs import optimize_params, compute_mse_scores, compute_baseline_mse, compute_mse_with_added_feature, extract_tsfresh_features, evaluate_feature, compute_mse_with_dropped_feature, drop_features\n",
    "# Define the target\n",
    "target = 'EXPGS'  # TESTING, PRESET TARGET\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "os.environ['PYTHONHASHSEED'] = str(42)\n",
    "\n",
    "def get_rounds_arg():\n",
    "    return 1\n",
    "# Define number of feature engineering rounds\n",
    "engineering_rounds = get_rounds_arg()\n",
    "\n",
    "# Load data\n",
    "X_train_transformed = pd.read_csv('data/processed/X_train_transformed.csv', index_col='Date', parse_dates=True)\n",
    "X_test_transformed = pd.read_csv('data/processed/X_test_transformed.csv', index_col='Date', parse_dates=True)\n",
    "y_train_transformed = pd.read_csv('data/processed/y_train_transformed.csv', index_col='Date', parse_dates=True)\n",
    "y_test_transformed = pd.read_csv('data/processed/y_test_transformed.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Combine X and y dataframes for feature engineering\n",
    "train_combined = pd.concat([X_train_transformed, y_train_transformed], axis=1)\n",
    "test_combined = pd.concat([X_test_transformed, y_test_transformed], axis=1)\n",
    "\n",
    "# Ensure the column names are preserved\n",
    "base_features = list(X_train_transformed.columns)\n",
    "\n",
    "# ADDING FEATURES, PT.2\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Suppress Optuna logging\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# Initial baseline MSE calculation\n",
    "baseline_mse_scores, aggregated_baseline_mse, best_params = compute_mse_scores(\n",
    "    X_train_transformed, X_test_transformed, y_train_transformed, y_test_transformed, base_features\n",
    ")\n",
    "\n",
    "print(f\"Baseline MSE Scores:\")\n",
    "print(baseline_mse_scores)\n",
    "print(f\"Aggregated Baseline MSE Score: {aggregated_baseline_mse}\\n\")\n",
    "\n",
    "initial_baseline_mse = aggregated_baseline_mse \n",
    "\n",
    "initial_mse_xgboost = baseline_mse_scores['XGBoost']\n",
    "initial_mse_lightgbm = baseline_mse_scores['LightGBM']\n",
    "\n",
    "# Save best params as dictionaries\n",
    "xgboost_params = best_params['XGBoost']\n",
    "lightgbm_params = best_params['LightGBM']\n",
    "\n",
    "# Save to a Python file\n",
    "with open('best_params.py', 'w') as f:\n",
    "    f.write(f\"xgboost_params = {xgboost_params}\\n\")\n",
    "    f.write(f\"lightgbm_params = {lightgbm_params}\\n\")\n",
    "\n",
    "print(\"Best parameters saved to best_params.py.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No features were dropped as they did not improve the model.\n",
      "Feature Dropping Completed.\n",
      "\n",
      "THRESHOLD OF 0.002:\n",
      "Initial MSE for XGBoost: 0.0005103063958695216\n",
      "Final MSE for XGBoost: 0.000526697588164481\n",
      "Improvement in MSE for XGBoost: -1.63911922949594e-05\n",
      "\n",
      "Initial MSE for LightGBM: 0.0005872492303105134\n",
      "Final MSE for LightGBM: 0.0006672955849259226\n",
      "Improvement in MSE for LightGBM: -8.004635461540926e-05\n"
     ]
    }
   ],
   "source": [
    "# V.1.0.0.0 - FEATURE DROPPING - USING THE THREE AT-A-TIME METHOD\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Set a higher threshold for improvement (e.g., 0.05% of the initial baseline MSE)\n",
    "threshold = 0.0002 * ((baseline_mse_scores['XGBoost'] + baseline_mse_scores['LightGBM']) / 2)\n",
    "\n",
    "# Function to compute MSE scores after dropping a feature\n",
    "def compute_mse_with_dropped_feature(X_train, X_test, y_train, y_test, base_features, drop_feature):\n",
    "    X_train = X_train[[f for f in base_features if f != drop_feature]].dropna().values\n",
    "    X_test = X_test[[f for f in base_features if f != drop_feature]].dropna().values\n",
    "    y_train = y_train.dropna().values.ravel()\n",
    "    y_test = y_test.dropna().values.ravel()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    mse_scores = {'XGBoost': [], 'LightGBM': []}\n",
    "\n",
    "    models = {\n",
    "        'XGBoost': XGBRegressor(**xgboost_params, verbosity=0),\n",
    "        'LightGBM': LGBMRegressor(**lightgbm_params, verbosity=-1)\n",
    "    }\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        mse_scores[model_name].append(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    mse_scores = {model: np.mean(scores) for model, scores in mse_scores.items()}\n",
    "    aggregated_mse = sum(mse_scores.values())\n",
    "\n",
    "    return mse_scores, aggregated_mse\n",
    "\n",
    "# Function to drop features\n",
    "def drop_features(train_combined, target, base_features,aggregated_baseline_mse,threshold):\n",
    "    aggregated_mse_scores_dropped = []\n",
    "    for feature in base_features:\n",
    "        mse_scores, aggregated_mse = compute_mse_with_dropped_feature(X_train_transformed, X_test_transformed, y_train_transformed, y_test_transformed, base_features, feature)\n",
    "        improvement = aggregated_baseline_mse - aggregated_mse\n",
    "        improvement_status = \"improved\" if improvement > threshold else \"worsened\"\n",
    "        aggregated_mse_scores_dropped.append((feature, aggregated_mse, improvement, improvement_status, mse_scores))\n",
    "\n",
    "\n",
    "    # Sort and drop the least impactful features if they result in improvement\n",
    "    aggregated_mse_scores_dropped.sort(key=lambda x: x[1])\n",
    "    features_to_drop = [f for f in aggregated_mse_scores_dropped if f[2] > threshold]\n",
    "\n",
    "    if not features_to_drop:\n",
    "        print(\"No features were dropped as they did not improve the model.\")\n",
    "    else:\n",
    "        for feature, _, improvement, _, _ in features_to_drop:\n",
    "            base_features.remove(feature)\n",
    "            print(f\"Feature dropped: {feature}, Improvement: {improvement}\")\n",
    "\n",
    "    print(\"Feature Dropping Completed.\")\n",
    "\n",
    "# Example usage:\n",
    "drop_features(train_combined, target, base_features,threshold,aggregated_baseline_mse)\n",
    "\n",
    "# Final baseline MSE calculation\n",
    "final_mse_scores, aggregated_final_mse, _ = compute_mse_scores(\n",
    "    X_train_transformed, X_test_transformed, y_train_transformed, y_test_transformed, base_features\n",
    ")\n",
    "# Store final MSE scores for each model\n",
    "final_mse_xgboost = final_mse_scores['XGBoost']\n",
    "final_mse_lightgbm = final_mse_scores['LightGBM']\n",
    "\n",
    "# Calculate improvements\n",
    "improvement_xgboost = initial_mse_xgboost - final_mse_xgboost\n",
    "improvement_lightgbm = initial_mse_lightgbm - final_mse_lightgbm\n",
    "\n",
    "# Print the results\n",
    "print(\"\")\n",
    "print(\"THRESHOLD OF 0.002:\")\n",
    "print(f\"Initial MSE for XGBoost: {initial_mse_xgboost}\")\n",
    "print(f\"Final MSE for XGBoost: {final_mse_xgboost}\")\n",
    "print(f\"Improvement in MSE for XGBoost: {improvement_xgboost}\\n\")\n",
    "\n",
    "print(f\"Initial MSE for LightGBM: {initial_mse_lightgbm}\")\n",
    "print(f\"Final MSE for LightGBM: {final_mse_lightgbm}\")\n",
    "print(f\"Improvement in MSE for LightGBM: {improvement_lightgbm}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogluon_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
