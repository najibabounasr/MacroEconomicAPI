{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DVC remote 'origin' set to https://dagshub.com/najibabounasr/MacroEconomicAPI.dvc\n"
     ]
    }
   ],
   "source": [
    "from funcs.dvc_funcs import dagshub_initialization, run_dvc_command\n",
    "\n",
    "# Initialize DVC and MLflow setup\n",
    "dagshub_initialization()\n",
    "\n",
    "# Now proceed with the rest of your script\n",
    "# # Example:\n",
    "# import mlflow\n",
    "\n",
    "# # Set MLflow experiment\n",
    "# mlflow.set_tracking_uri(\"https://dagshub.com/najibabounasr/Macro_Economic_Forecast_Stage_2.mlflow\")\n",
    "# mlflow.set_experiment(\"YourExperimentName\")\n",
    "\n",
    "# # Your further code logic here...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DVC remote 'origin' set to https://dagshub.com/najibabounasr/MacroEconomicAPI.dvc\n",
      "Fetching data for FEDFUNDS\n",
      "Fetching data for GDP\n",
      "Fetching data for CPIAUCSL\n",
      "Fetching data for CUSR0000SAH1\n",
      "Fetching data for CPILFESL\n",
      "Fetching data for PCE\n",
      "Fetching data for PRFI\n",
      "Fetching data for PNFI\n",
      "Fetching data for EXPGS\n",
      "Fetching data for HOUST\n",
      "Fetching data for DSPI\n",
      "Fetching data for DGS2\n",
      "Fetching data for DGS5\n",
      "Fetching data for DGS10\n",
      "Fetching data for AAA\n",
      "Fetching data for BAA\n",
      "Fetching data for WTISPLC\n",
      "Fetching data for IMPGS\n",
      "Fetching data for GCE\n",
      "Fetching data for FGCE\n",
      "Fetching data for GDPCTPI\n",
      "Fetching data for PCEPI\n",
      "Fetching data for PCEPILFE\n",
      "Fetching data for PAYEMS\n",
      "Fetching data for UNRATE\n",
      "Fetching data for INDPRO\n",
      "Fetching data for CUMFNS\n",
      "Fetching data for USREC\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Client created. Use the name of the repo <span style=\"font-weight: bold\">(</span>MacroEconomicAPI<span style=\"font-weight: bold\">)</span> as the name of the bucket\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Client created. Use the name of the repo \u001b[1m(\u001b[0mMacroEconomicAPI\u001b[1m)\u001b[0m as the name of the bucket\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fetching complete and tracked with DVC and Dagshub.\n"
     ]
    }
   ],
   "source": [
    "from funcs.process_data_funcs import fetch_data\n",
    "import pandas as pd\n",
    "from local_settings import settings\n",
    "from fredapi import Fred\n",
    "import os\n",
    "from funcs.dvc_funcs import run_dvc_command, dagshub_initialization, upload_to_dagshub\n",
    "from dagshub import get_repo_bucket_client\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fetch_all_data(target_feature):\n",
    "    # Initialize FRED API with your API key\n",
    "    fred = Fred(api_key=settings['api_key'])\n",
    "\n",
    "    # Fetch and store data for each series in a dictionary\n",
    "    data_frames = {}\n",
    "    for series_id in settings['series_ids']:\n",
    "        frequency = settings['frequency_map'].get(series_id, 'm')  # Default to 'm' if not specified\n",
    "        try:\n",
    "            data_frame = fetch_data(series_id, frequency)\n",
    "            if not data_frame.empty:\n",
    "                data_frames[series_id] = data_frame\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {series_id}: {e}\")\n",
    "\n",
    "    # Combine all data into a single DataFrame\n",
    "    combined_data = pd.concat(data_frames.values(), axis=1, keys=data_frames.keys())\n",
    "    combined_data = combined_data.asfreq('MS')\n",
    "    combined_data.index.name = 'Date'\n",
    "\n",
    "    # Drop a level from the multi-level columns\n",
    "    combined_data.columns = combined_data.columns.droplevel(1)\n",
    "\n",
    "    # Ensure the target feature is included in the dataset\n",
    "    if target_feature not in combined_data.columns:\n",
    "        valid_features = \", \".join(combined_data.columns)\n",
    "        raise ValueError(f\"Target feature '{target_feature}' is not available in the dataset. Valid features are: {valid_features}\")\n",
    "\n",
    "    # Save raw data locally\n",
    "    if not os.path.exists('data/raw'):\n",
    "        os.makedirs('data/raw')\n",
    "    combined_data.to_csv('data/raw/raw_data.csv')\n",
    "\n",
    "    # Upload raw data to Dagshub\n",
    "    upload_to_dagshub('data/raw/raw_data.csv', 'data/raw/raw_data.csv')\n",
    "\n",
    "    print(\"Data fetching complete and tracked with DVC and Dagshub.\")\n",
    "    return combined_data\n",
    "\n",
    "def main():\n",
    "    dagshub_initialization()\n",
    "    # This function is used to test the script independently\n",
    "    target_feature = 'GDP'  # Example target feature\n",
    "    fetch_all_data(target_feature)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DVC remote 'origin' set to https://dagshub.com/najibabounasr/MacroEconomicAPI.dvc\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['--f=c:\\\\\\\\Users\\\\\\\\nabounaser\\\\\\\\AppData\\\\\\\\Roaming\\\\\\\\jupyter\\\\\\\\runtime\\\\\\\\kernel-v2-15444EWxWh1m3A8cQ.json'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 127\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo target feature provided. Please specify the target feature.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    126\u001b[0m target \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 127\u001b[0m \u001b[43mprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[30], line 18\u001b[0m, in \u001b[0;36mprocess_data\u001b[1;34m(target)\u001b[0m\n\u001b[0;32m     15\u001b[0m combined_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/raw/raw_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, parse_dates\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Split data into X (features) and y (target)\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m y \u001b[38;5;241m=\u001b[39m combined_data[[target]]\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Split the data into train and test sets\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nabounaser\\AppData\\Local\\miniconda3\\envs\\MacroEconomicForecast\\Lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nabounaser\\AppData\\Local\\miniconda3\\envs\\MacroEconomicForecast\\Lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\nabounaser\\AppData\\Local\\miniconda3\\envs\\MacroEconomicForecast\\Lib\\site-packages\\pandas\\core\\generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\nabounaser\\AppData\\Local\\miniconda3\\envs\\MacroEconomicForecast\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['--f=c:\\\\\\\\Users\\\\\\\\nabounaser\\\\\\\\AppData\\\\\\\\Roaming\\\\\\\\jupyter\\\\\\\\runtime\\\\\\\\kernel-v2-15444EWxWh1m3A8cQ.json'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from funcs.process_data_funcs import (\n",
    "    impute_missing_values_spline, deflate_nominal_values, apply_log_transformations,\n",
    "    apply_best_transformations, cap_outliers\n",
    ")\n",
    "from funcs.dvc_funcs import dagshub_initialization\n",
    "from dagshub import get_repo_bucket_client\n",
    "\n",
    "def process_data(target):\n",
    "    # Load combined data from CSV\n",
    "    combined_data = pd.read_csv('data/raw/raw_data.csv', parse_dates=True, index_col='Date')\n",
    "\n",
    "    # Split data into X (features) and y (target)\n",
    "    X = combined_data.drop(columns=[target])\n",
    "    y = combined_data[[target]]\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "    # Impute missing values using spline interpolation\n",
    "    quarterly_columns = ['GDP', 'PRFI', 'PNFI', 'EXPGS', 'IMPGS', 'GCE', 'FGCE', 'GDPCTPI']\n",
    "    treasury_yield_columns = ['DGS2', 'DGS5', 'DGS10']\n",
    "\n",
    "    for column in quarterly_columns + treasury_yield_columns:\n",
    "        if column in X_train.columns:\n",
    "            X_train = impute_missing_values_spline(X_train, column)\n",
    "            X_test = impute_missing_values_spline(X_test, column)\n",
    "        elif column in y_train.columns:\n",
    "            y_train = impute_missing_values_spline(y_train, column)\n",
    "            y_test = impute_missing_values_spline(y_test, column)\n",
    "\n",
    "    # Ensure index column is named 'Date'\n",
    "    for df in [X_train, X_test, y_train, y_test]:\n",
    "        df.index.name = 'Date'\n",
    "\n",
    "    # Combine X and y dataframes\n",
    "    train_combined = pd.concat([X_train, y_train], axis=1)\n",
    "    test_combined = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    # Ensure CPIAUCSL is included in the dataframes\n",
    "    cpi_col_name = 'CPIAUCSL'\n",
    "    columns_to_deflate = ['GDP', 'PCE', 'PRFI', 'PNFI', 'EXPGS', 'IMPGS', 'GCE', 'FGCE', 'DSPI']\n",
    "\n",
    "    # Deflate target column if it's in the columns to deflate\n",
    "    if target in columns_to_deflate:\n",
    "        deflated_train = deflate_nominal_values(train_combined[[target, cpi_col_name]], cpi_col_name, [target])\n",
    "        deflated_test = deflate_nominal_values(test_combined[[target, cpi_col_name]], cpi_col_name, [target])\n",
    "        train_combined[target] = deflated_train[target]\n",
    "        test_combined[target] = deflated_test[target]\n",
    "        columns_to_deflate.remove(target)\n",
    "\n",
    "    # Deflate remaining columns\n",
    "    train_combined = deflate_nominal_values(train_combined, cpi_col_name, columns_to_deflate)\n",
    "    test_combined = deflate_nominal_values(test_combined, cpi_col_name, columns_to_deflate)\n",
    "\n",
    "    # Apply logarithmic transformations\n",
    "    columns_to_transform = ['GDP', 'PCE', 'PRFI', 'PNFI', 'EXPGS', 'IMPGS', 'GCE', 'FGCE', 'HOUST', 'DSPI']\n",
    "\n",
    "    if target in columns_to_transform:\n",
    "        train_combined[target] = apply_log_transformations(train_combined[[target]], [target])\n",
    "        test_combined[target] = apply_log_transformations(test_combined[[target]], [target])\n",
    "        columns_to_transform.remove(target)\n",
    "\n",
    "    train_combined = apply_log_transformations(train_combined, columns_to_transform)\n",
    "    test_combined = apply_log_transformations(test_combined, columns_to_transform)\n",
    "\n",
    "    # Standardize/Normalize the Data\n",
    "    scaler = StandardScaler()\n",
    "    X_train[X_train.columns] = scaler.fit_transform(X_train)\n",
    "    X_test[X_test.columns] = scaler.transform(X_test)\n",
    "    y_train[target] = scaler.fit_transform(y_train[target].values.reshape(-1, 1))\n",
    "    y_test[target] = scaler.transform(y_test[target].values.reshape(-1, 1))\n",
    "\n",
    "    # Apply Percentage Change\n",
    "    X_train_pct_change = X_train.pct_change().dropna()\n",
    "    X_test_pct_change = X_test.pct_change().dropna()\n",
    "    y_train_pct_change = y_train.pct_change().dropna()\n",
    "    y_test_pct_change = y_test.pct_change().dropna()\n",
    "\n",
    "    # Apply the best transformations\n",
    "    X_train_transformed = apply_best_transformations(X_train_pct_change)\n",
    "    X_test_transformed = apply_best_transformations(X_test_pct_change)\n",
    "    y_train_transformed = apply_best_transformations(y_train_pct_change)\n",
    "    y_test_transformed = apply_best_transformations(y_test_pct_change)\n",
    "\n",
    "    # Handle outliers in the transformed data\n",
    "    X_train_transformed = cap_outliers(X_train_transformed, cap_factor=3.0)\n",
    "    y_train_transformed = cap_outliers(y_train_transformed, cap_factor=3.0)\n",
    "\n",
    "    # Combine X and y after final transformations\n",
    "    train_transformed_combined = pd.concat([X_train_transformed, y_train_transformed], axis=1)\n",
    "    test_transformed_combined = pd.concat([X_test_transformed, y_test_transformed], axis=1)\n",
    "\n",
    "    # Save the transformed data locally\n",
    "    train_transformed_combined.to_csv('data/processed/train_transformed_combined.csv', index=True)\n",
    "    test_transformed_combined.to_csv('data/processed/test_transformed_combined.csv', index=True)\n",
    "\n",
    "    # Upload to Dagshub storage\n",
    "    s3 = get_repo_bucket_client(\"najibabounasr/MacroEconomicAPI\")\n",
    "    s3.upload_file(\n",
    "        Bucket=\"MacroEconomicAPI\",\n",
    "        Filename=\"data/processed/train_transformed_combined.csv\",\n",
    "        Key=\"train_transformed_combined.csv\",\n",
    "    )\n",
    "    s3.upload_file(\n",
    "        Bucket=\"MacroEconomicAPI\",\n",
    "        Filename=\"data/processed/test_transformed_combined.csv\",\n",
    "        Key=\"test_transformed_combined.csv\",\n",
    "    )\n",
    "\n",
    "    # Save individual transformed datasets locally\n",
    "    X_train_transformed.to_csv('data/processed/X_train_transformed.csv', index=True)\n",
    "    X_test_transformed.to_csv('data/processed/X_test_transformed.csv', index=True)\n",
    "    y_train_transformed.to_csv('data/processed/y_train_transformed.csv', index=True)\n",
    "    y_test_transformed.to_csv('data/processed/y_test_transformed.csv', index=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    dagshub_initialization()\n",
    "    if len(sys.argv) < 2:\n",
    "        raise ValueError(\"No target feature provided. Please specify the target feature.\")\n",
    "    target = sys.argv[1]\n",
    "    process_data(target)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogluon_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
